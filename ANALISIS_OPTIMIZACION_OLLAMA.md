# üîç An√°lisis de Propuesta de Optimizaci√≥n Ollama

## ‚ùå Problema Identificado en las Pruebas

**Error**: `cannot unpack non-iterable bool object`

**Causa**: Las funciones `validate_fiscal_rules()` y `validate_business_rules()` retornan solo `bool`, no tupla `(bool, str)`. El script de prueba intentaba desempaquetar como tupla.

**Estado**: ‚úÖ **CORREGIDO** - El script ahora maneja correctamente los retornos booleanos.

---

## üìä An√°lisis de la Propuesta de Optimizaci√≥n

### ‚úÖ **1. Reducir context window (num_ctx: 2048)**

**Opci√≥n**: Excelente ‚úÖ

**Raz√≥n**:
- Para OCR de facturas no necesitamos tanto contexto
- Reduce memoria de ~7GB ‚Üí ~4.5GB seg√∫n la propuesta
- Compatible con nuestra factura JSON que es corta

**Implementaci√≥n**:
- A√±adir a `payload` en `ocr_extractor.py`: `"options": {"num_ctx": 2048}`
- O en `.env`: `OLLAMA_NUM_CTX=2048` (si Ollama lo soporta como env)

**Recomendaci√≥n**: ‚úÖ **IMPLEMENTAR**

---

### ‚úÖ **2. Limitar threads de CPU (num_thread: 4)**

**Opci√≥n**: Muy buena ‚úÖ

**Raz√≥n**:
- En servidor de 8GB con PostgreSQL y otros servicios corriendo
- Evita saturaci√≥n del sistema
- Recomendado: `num_thread: 2` o `3` para dejar margen

**Implementaci√≥n**:
- A√±adir a `payload`: `"options": {"num_thread": 2}`
- O verificar con `nproc` cu√°ntos cores tenemos disponibles

**Recomendaci√≥n**: ‚úÖ **IMPLEMENTAR** (con valor conservador: 2-3 threads)

---

### ‚ö†Ô∏è **3. low_vram y f16_kv en modelo**

**Opci√≥n**: Potencialmente √∫til, pero requiere verificaci√≥n ‚ö†Ô∏è

**Raz√≥n**:
- `low_vram: true` y `f16_kv: true` son par√°metros de modelo/compilaci√≥n
- Pueden requerir recrear el modelo o configurar en `Modelfile`
- No todos los par√°metros est√°n disponibles en la API `/api/generate`

**Implementaci√≥n**:
- Verificar si Ollama soporta estos par√°metros v√≠a API `options`
- Alternativa: Crear modelo custom con `ollama create` y `Modelfile`

**Recomendaci√≥n**: ‚ö†Ô∏è **VERIFICAR PRIMERO** si funciona v√≠a API antes de crear modelo custom

---

### ‚úÖ **4. Cuantizaci√≥n 4-bit (Q4_K_M)**

**Opci√≥n**: Excelente para reducir memoria ‚úÖ

**Raz√≥n**:
- Reduce modelo de 4.7GB ‚Üí ~3.2GB
- P√©rdida de precisi√≥n m√≠nima para OCR
- Compatible con nuestro caso de uso

**Implementaci√≥n**:
```bash
# Verificar si existe versi√≥n cuantizada
ollama pull llava:7b-q4_K_M

# O crear modelo custom
ollama create llava7b-q4 -f Modelfile
# Modelfile:
# FROM llava:7b
# PARAMETER quantize q4_K_M
```

**Recomendaci√≥n**: ‚úÖ **IMPLEMENTAR** - Es la optimizaci√≥n m√°s efectiva

---

### ‚úÖ **5. Limitar tokens de salida (num_predict: 200)**

**Opci√≥n**: Muy buena ‚úÖ

**Raz√≥n**:
- Nuestro JSON de salida es corto (~100-150 tokens)
- Evita procesamiento innecesario
- Reduce tiempo de respuesta

**Implementaci√≥n**:
- A√±adir a `payload`: `"options": {"num_predict": 200}`

**Recomendaci√≥n**: ‚úÖ **IMPLEMENTAR**

---

### ‚úÖ **6. Procesamiento secuencial**

**Opci√≥n**: Ya implementado ‚úÖ

**Estado**: 
- ‚úÖ El c√≥digo actual procesa facturas una por una
- ‚úÖ No usa ThreadPoolExecutor concurrente
- ‚úÖ Cada request espera respuesta antes de siguiente

**Recomendaci√≥n**: ‚úÖ **MANTENER** - Ya est√° bien implementado

---

### ‚úÖ **7. Monitoreo en tiempo real**

**Opci√≥n**: √ötil para diagn√≥stico ‚úÖ

**Implementaci√≥n**:
```bash
# Ver memoria de Ollama
watch -n 2 "ps aux | grep ollama | grep -v grep | awk '{print \$4, \$11}'"

# O con htop
htop
```

**Recomendaci√≥n**: ‚úÖ **USAR** para monitorear durante pruebas

---

## üéØ Plan de Implementaci√≥n Recomendado

### Fase 1: Optimizaciones inmediatas (v√≠a API)

1. ‚úÖ A√±adir `num_ctx: 2048` en payload
2. ‚úÖ A√±adir `num_thread: 2` en payload  
3. ‚úÖ A√±adir `num_predict: 200` en payload

**Impacto esperado**: 
- Memoria: ~7GB ‚Üí ~4.5GB
- Velocidad: 20-25s ‚Üí 15-18s por factura

### Fase 2: Optimizaci√≥n de modelo (requiere recrear)

4. ‚úÖ Probar cuantizaci√≥n Q4_K_M (requiere nuevo modelo)

**Impacto esperado**:
- Memoria: ~4.5GB ‚Üí ~3.2GB
- Velocidad: Similar o ligeramente m√°s lento
- Precisi√≥n: M√≠nima p√©rdida (aceptable para OCR)

### Fase 3: Verificaci√≥n avanzada (opcional)

5. ‚ö†Ô∏è Investigar `low_vram` y `f16_kv` v√≠a API o Modelfile

---

## üìù Par√°metros Recomendados para Nuestro Caso

```python
payload = {
    "model": "llava:7b",
    "prompt": prompt,
    "images": [image_base64],
    "format": "json",
    "stream": False,
    "options": {
        "num_ctx": 2048,        # Reducir contexto
        "num_thread": 2,         # Limitar threads (conservador)
        "num_predict": 200,      # Limitar salida
        # "low_vram": True,      # Verificar si funciona
        # "f16_kv": True         # Verificar si funciona
    }
}
```

---

## üö® Notas Importantes

1. **num_thread**: Empezar con valor conservador (2). Si el sistema responde bien, podemos aumentar a 3-4.

2. **Cuantizaci√≥n**: Requiere descargar/crear nuevo modelo. Asegurarse de tener espacio en disco.

3. **Validaci√≥n**: Probar cada cambio individualmente para medir impacto real.

4. **Compatibility**: Algunos par√°metros pueden no estar disponibles en todas las versiones de Ollama. Verificar con `ollama --version`.

---

## ‚úÖ Conclusi√≥n

**Implementar ahora**:
- ‚úÖ num_ctx: 2048
- ‚úÖ num_thread: 2  
- ‚úÖ num_predict: 200

**Probar despu√©s**:
- ‚úÖ Cuantizaci√≥n Q4_K_M
- ‚ö†Ô∏è low_vram / f16_kv (si est√°n disponibles)

**Ya implementado**:
- ‚úÖ Procesamiento secuencial

**Impacto esperado total**: Reducci√≥n de memoria de ~7GB ‚Üí ~3-3.5GB, con mejora en velocidad del 20-30%.



