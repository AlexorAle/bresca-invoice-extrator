#!/usr/bin/env python3
"""
Script ejecutable para ingesta incremental desde Google Drive
Se puede ejecutar desde cron o manualmente

Uso:
    python scripts/run_ingest_incremental.py
    
    # Con par√°metros opcionales:
    python scripts/run_ingest_incremental.py --folder-id FOLDER_ID --batch-size 20
    
    # Solo validar (dry-run):
    python scripts/run_ingest_incremental.py --dry-run
"""
import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime

# Agregar ra√≠z del proyecto al path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.security.secrets import load_env
from src.pipeline.ingest_incremental import IncrementalIngestPipeline
from src.pipeline.job_lock import JobLock
from src.drive.drive_incremental import DriveIncrementalClient
from src.sync.state_store import get_state_store
from src.db.database import Database
from src.logging_conf import get_logger
from filelock import Timeout

# Cargar variables de entorno
load_env()

logger = get_logger(__name__)


def parse_args():
    """Parsear argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Ingesta incremental de facturas desde Google Drive'
    )
    
    parser.add_argument(
        '--folder-id',
        type=str,
        help='ID de carpeta Drive objetivo (por defecto desde GOOGLE_DRIVE_FOLDER_ID)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        help='N√∫mero de archivos por lote (por defecto desde BATCH_SIZE)'
    )
    
    parser.add_argument(
        '--max-pages',
        type=int,
        help='M√°ximo de p√°ginas Drive a procesar (por defecto desde MAX_PAGES_PER_RUN)'
    )
    
    parser.add_argument(
        '--sleep-between-batch',
        type=int,
        help='Segundos entre lotes (por defecto desde SLEEP_BETWEEN_BATCH_SEC)'
    )
    
    parser.add_argument(
        '--advance-strategy',
        type=str,
        choices=['MAX_OK_TIME', 'CURRENT_TIME'],
        help='Estrategia para avanzar timestamp (por defecto desde ADVANCE_STRATEGY)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Solo validar configuraci√≥n y mostrar archivos a procesar, sin procesarlos'
    )
    
    parser.add_argument(
        '--output-json',
        type=str,
        help='Guardar estad√≠sticas en archivo JSON'
    )
    
    parser.add_argument(
        '--reset-state',
        action='store_true',
        help='PELIGRO: Resetear √∫ltimo timestamp de sincronizaci√≥n (forzar rescan completo)'
    )
    
    return parser.parse_args()


def print_banner(title: str):
    """Imprimir banner decorado"""
    print()
    print("=" * 80)
    print(f"  {title}")
    print("=" * 80)
    print()


def validate_configuration() -> bool:
    """
    Validar que la configuraci√≥n est√° completa
    
    Returns:
        True si la configuraci√≥n es v√°lida, False en caso contrario
    """
    print_banner("VALIDACI√ìN DE CONFIGURACI√ìN")
    
    required_vars = [
        'GOOGLE_SERVICE_ACCOUNT_FILE',
        'GOOGLE_DRIVE_FOLDER_ID',
        'DATABASE_URL',
        'STATE_BACKEND'
    ]
    
    missing = []
    
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            missing.append(var)
            print(f"‚ùå {var}: NO CONFIGURADA")
        else:
            # Ocultar valores sensibles
            if 'PASSWORD' in var or 'SECRET' in var or 'KEY' in var:
                display_value = "***"
            elif 'URL' in var:
                display_value = value[:30] + "..." if len(value) > 30 else value
            else:
                display_value = value
            
            print(f"‚úÖ {var}: {display_value}")
    
    if missing:
        print()
        print(f"‚ùå Faltan {len(missing)} variables de entorno requeridas")
        return False
    
    print()
    print("‚úÖ Configuraci√≥n v√°lida")
    return True


def dry_run_info(pipeline: IncrementalIngestPipeline):
    """
    Mostrar informaci√≥n de dry-run sin procesar
    
    Args:
        pipeline: Instancia del pipeline
    """
    print_banner("DRY RUN - INFORMACI√ìN")
    
    # Obtener estado actual
    last_sync_time = pipeline.state_store.get_last_sync_time()
    
    print(f"üìÅ Carpeta objetivo: {pipeline.folder_id}")
    print(f"‚è∞ √öltima sincronizaci√≥n: {last_sync_time.isoformat() if last_sync_time else 'N/A (primera ejecuci√≥n)'}")
    print(f"üì¶ Tama√±o de lote: {pipeline.batch_size}")
    print(f"üìÑ M√°ximo de p√°ginas: {pipeline.max_pages_per_run}")
    print(f"‚è±Ô∏è  Pausa entre lotes: {pipeline.sleep_between_batch}s")
    print(f"üîÑ Estrategia de avance: {pipeline.advance_strategy}")
    print()
    
    # Validar acceso a carpeta
    print("üîç Validando acceso a carpeta...")
    if not pipeline.drive_client.validate_folder_access(pipeline.folder_id):
        print("‚ùå No se puede acceder a la carpeta")
        return
    
    print("‚úÖ Acceso validado")
    print()
    
    # Contar archivos a procesar
    print("üîç Contando archivos a procesar...")
    try:
        count = pipeline.drive_client.get_file_count_since(
            pipeline.folder_id,
            last_sync_time
        )
        
        print(f"üìä Archivos a procesar: {count}")
        
        if count == 0:
            print("‚ÑπÔ∏è  No hay archivos nuevos o modificados desde la √∫ltima sincronizaci√≥n")
        elif count > 100:
            print(f"‚ö†Ô∏è  Se encontraron muchos archivos ({count}). Considerar ejecutar en lotes.")
        
    except Exception as e:
        print(f"‚ùå Error contando archivos: {e}")


def reset_sync_state(db: Database):
    """
    Resetear estado de sincronizaci√≥n (PELIGRO)
    
    Args:
        db: Instancia de Database
    """
    print_banner("‚ö†Ô∏è  RESETEAR ESTADO DE SINCRONIZACI√ìN")
    
    print("ADVERTENCIA: Esta acci√≥n eliminar√° el timestamp de √∫ltima sincronizaci√≥n.")
    print("La pr√≥xima ejecuci√≥n procesar√° todos los archivos en la ventana de tiempo configurada.")
    print()
    
    response = input("¬øEst√° seguro? Escriba 'RESETEAR' para confirmar: ")
    
    if response != 'RESETEAR':
        print("‚ùå Operaci√≥n cancelada")
        return
    
    try:
        from src.db.repositories import SyncStateRepository
        repo = SyncStateRepository(db)
        repo.delete_value('drive_last_sync_time')
        
        print("‚úÖ Estado de sincronizaci√≥n reseteado")
    
    except Exception as e:
        print(f"‚ùå Error reseteando estado: {e}")


def main():
    """Funci√≥n principal"""
    args = parse_args()
    
    print_banner("üöÄ INGESTA INCREMENTAL - GOOGLE DRIVE")
    
    print(f"‚è∞ Inicio: {datetime.utcnow().isoformat()}Z")
    print(f"üíª Host: {os.uname().nodename if hasattr(os, 'uname') else 'unknown'}")
    print()
    
    # Validar configuraci√≥n
    if not validate_configuration():
        print()
        print("‚ùå Configuraci√≥n inv√°lida. Abortando.")
        sys.exit(1)
    
    # Inicializar database
    try:
        db = Database()
        logger.info("Base de datos inicializada")
    except Exception as e:
        print(f"‚ùå Error inicializando base de datos: {e}")
        sys.exit(1)
    
    # Manejar reset de estado
    if args.reset_state:
        reset_sync_state(db)
        sys.exit(0)
    
    # Construir kwargs para pipeline
    pipeline_kwargs = {}
    
    if args.folder_id:
        pipeline_kwargs['folder_id'] = args.folder_id
    
    if args.batch_size:
        pipeline_kwargs['batch_size'] = args.batch_size
    
    if args.max_pages:
        pipeline_kwargs['max_pages_per_run'] = args.max_pages
    
    if args.sleep_between_batch:
        pipeline_kwargs['sleep_between_batch'] = args.sleep_between_batch
    
    if args.advance_strategy:
        pipeline_kwargs['advance_strategy'] = args.advance_strategy
    
    # Inicializar pipeline
    try:
        pipeline = IncrementalIngestPipeline(**pipeline_kwargs)
    except Exception as e:
        print(f"‚ùå Error inicializando pipeline: {e}")
        logger.error(f"Error inicializando pipeline: {e}", exc_info=True)
        sys.exit(1)
    
    # Dry run
    if args.dry_run:
        dry_run_info(pipeline)
        print()
        print("‚ÑπÔ∏è  Dry run completado. No se procesaron archivos.")
        sys.exit(0)
    
    # Verificar lock antes de ejecutar (para dar mensaje m√°s claro)
    job_lock = JobLock()
    if job_lock.is_locked():
        print()
        print("‚ö†Ô∏è  ADVERTENCIA: Otra instancia del job est√° ejecut√°ndose")
        print(f"   Lock file: {job_lock.lock_file_path}")
        print()
        print("   Si est√°s seguro de que no hay otra instancia ejecut√°ndose,")
        print("   puedes forzar la liberaci√≥n del lock (peligroso):")
        print("   python -c 'from src.pipeline.job_lock import JobLock; JobLock().force_release()'")
        print()
        sys.exit(1)
    
    # Ejecutar pipeline
    print_banner("‚öôÔ∏è  EJECUTANDO PIPELINE")
    
    try:
        stats = pipeline.run()
        
        # Mostrar resumen
        print_banner("üìä RESUMEN DE EJECUCI√ìN")
        
        print(f"‚è±Ô∏è  Duraci√≥n: {stats['duration_seconds']}s")
        print(f"üìÑ P√°ginas consultadas: {stats['drive_pages_fetched_total']}")
        print(f"üì• Archivos listados: {stats['drive_items_listed_total']}")
        print(f"üíæ Archivos descargados: {stats['files_downloaded']}")
        print()
        print(f"‚úÖ Procesados OK: {stats['invoices_processed_ok_total']}")
        print(f"üîÑ Revisiones: {stats['invoices_revision_total']}")
        print(f"üìã Duplicados: {stats['invoices_duplicate_total']}")
        print(f"‚ö†Ô∏è  Para revisi√≥n: {stats['invoices_review_total']}")
        print(f"üö´ Ignorados: {stats['invoices_ignored_total']}")
        print(f"‚ùå Errores: {stats['invoices_error_total']}")
        print()
        print(f"üïê Timestamp anterior: {stats['last_sync_time_before'] or 'N/A'}")
        print(f"üïë Timestamp nuevo: {stats['last_sync_time_after'] or 'Sin cambios'}")
        print()
        
        # Guardar JSON si se solicita
        if args.output_json:
            output_path = Path(args.output_json)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            print(f"üìÑ Estad√≠sticas guardadas en: {output_path}")
            print()
        
        # Determinar exit code
        if stats['invoices_error_total'] > 0 or stats['download_errors'] > 0:
            print("‚ö†Ô∏è  Ejecuci√≥n completada con errores parciales")
            exit_code = 2  # Errores parciales
        else:
            print("‚úÖ Ejecuci√≥n completada exitosamente")
            exit_code = 0
        
        print()
        print(f"‚è∞ Fin: {datetime.utcnow().isoformat()}Z")
        
        sys.exit(exit_code)
    
    except Timeout:
        print()
        print("‚ùå Error: Otra instancia del job est√° ejecut√°ndose")
        print("   Espera a que termine o verifica procesos activos")
        logger.error("Job bloqueado: otra instancia en ejecuci√≥n")
        sys.exit(1)
    
    except KeyboardInterrupt:
        print()
        print("‚ö†Ô∏è  Ejecuci√≥n interrumpida por usuario")
        logger.warning("Ejecuci√≥n interrumpida por usuario (KeyboardInterrupt)")
        sys.exit(130)
    
    except Exception as e:
        print()
        print(f"‚ùå Error cr√≠tico: {e}")
        logger.error(f"Error cr√≠tico en ejecuci√≥n: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()

